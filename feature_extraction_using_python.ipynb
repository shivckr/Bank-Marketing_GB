{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature extraction using python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOfJvHLj8sLOTXHIi+dJBeU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivckr/Bank-Marketing_GB/blob/master/feature_extraction_using_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOn1sE0gITea",
        "colab_type": "text"
      },
      "source": [
        "#loading dataste friom kaggle into colab\n",
        "Source: https://medium.com/@paritosh_30025/natural-language-processing-text-data-vectorization-af2520529cf7\n",
        "\n",
        "https://www.kaggle.com/shashanksai/text-preprocessing-using-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lXJk8B_JVzd",
        "colab_type": "text"
      },
      "source": [
        "We cannot work with the text data in machine learning so we need to convert them into numerical vectors\n",
        "\n",
        "Text data needs to be cleaned and encoded to numerical values before giving them to machine learning models, this process of cleaning and encoding is called as Text Preprocessing or vectorization of often featrure extraction.\n",
        "\n",
        "\n",
        "\n",
        "Understanding the data - See what's data is all about. what should be considered for cleaning for data (Punctuations , stopwords etc..).\n",
        "Basic Cleaning need to be considered for cleaning of data (like Punctuations , stopwords etc..) and its code.\n",
        "\n",
        "Text preprocessing techinique\n",
        "1. Removal of Noisy data  ( Removal XML, HTML, markup, header etc. use BeautifulSoup library for XML & HTML,  regular expression for others)\n",
        "2. Tokenization  (sentence into words. NLTK library)\n",
        "3. Normalization  (removing punctuation and stop word, lemmatization, stemming  etc )\n",
        "\n",
        "Techniques for vectorization - .\n",
        "\n",
        "1.   Bag of Words\n",
        "2.   Binary Bag of Words\n",
        "3.   trigram, Ngram\n",
        "4.   TF-IDF( Term Frequency - Inverse Document Frequency)\n",
        "5.   Google Word2Vec\n",
        "6.   Avg-Word2Vec\n",
        "7.   TF-IDF Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CRqcEJzrPm-",
        "colab_type": "code",
        "outputId": "4271ab4b-38ca-4a59-9bd2-ede63c28da1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#Mount the drive to colab notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I41d7jYjvaen",
        "colab_type": "code",
        "outputId": "88de7cf3-722a-4e50-9b47-5c3045d419e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# /content/gdrive/My Drive/Kaggle is the path where kaggle.json is present in the Google Drive\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
        "#changing the working directory\n",
        "%cd /content/gdrive/My Drive/Kaggle"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Kaggle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAg6sEoe06IO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3f84a101-3d2d-42ed-e422-b1365c39b5b4"
      },
      "source": [
        "# copy API and download dataset into your drive\n",
        "!kaggle datasets download -d snap/amazon-fine-food-reviews"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading amazon-fine-food-reviews.zip to /content/gdrive/My Drive/Kaggle\n",
            " 97% 234M/242M [00:03<00:00, 84.5MB/s]\n",
            "100% 242M/242M [00:04<00:00, 59.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qci5ABsb1RZH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ef425076-03e0-447b-ba82-07a20e45ccdd"
      },
      "source": [
        "#unzipping the zip files and deleting the zip files\n",
        "!unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  amazon-fine-food-reviews.zip\n",
            "replace Reviews.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace database.sqlite? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace hashes.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8Tkr7qW2EJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #In order to perform machine learning on text, we need to transform our documents into vector representations\n",
        "#  such that we can apply numeric machine learning. This process is called feature extraction or more simply, vectorization, \n",
        "#    and is an essential first step toward language-aware analysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vgw1r961_Eu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Read sql file with pandas \n",
        "import sqlite3                                               # to call query \n",
        "from  sqlalchemy import create_engine                       # to create the connection \n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-TW9wJS9amv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# df = pd.read_csv('Reviews.csv')\n",
        "# print(df.shape)\n",
        "# #df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIz5g_k6GDNy",
        "colab_type": "text"
      },
      "source": [
        "#How to load very big data with pandas \n",
        "source: https://www.youtube.com/watch?v=xKMyk4wDHnQ\n",
        "\n",
        "Three  steps:\n",
        "\n",
        "1. create a connector to a database\n",
        "       # csv_databse = create_engine('sqlite://csv_database.db')\n",
        "2. Building the database(load the csv file ) by chunking\n",
        "       # In this case we we already have .sql file.So, this skiping  this step\n",
        "3. construct the Pandas dataframe from database by sql query\n",
        "        pd.read_sql_query(\"select statement\")   \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUvzkig5_2V2",
        "colab_type": "code",
        "outputId": "60687ba1-98c7-4d63-985a-466633c62f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Read sqlite query results into a pandas DataFrame\n",
        "con = sqlite3.connect(\"database.sqlite\")\n",
        "sql_df = pd.read_sql_query(\"SELECT * from Reviews\", con)\n",
        "\n",
        "# Verify that result of SQL query is stored in the dataframe\n",
        "#print(df.head())\n",
        "print(sql_df.shape)\n",
        "con.close()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(568454, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wggdzh28USZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords                                                 #Stopwords corpus\n",
        "from nltk.stem import PorterStemmer                                               # Stemmer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer                       #For Bag of words\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer                       #For TF-IDF\n",
        "from gensim.models import Word2Vec                                                #For Word2Vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1pn0i-7AyHZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "283604e1-274a-4762-eca6-a20fc4d4fb2e"
      },
      "source": [
        "#print column of dataframe\n",
        "print(sql_df.columns)\n",
        "\n",
        "#print values present in column 'Score'\n",
        "print(sql_df.Score.unique())\n",
        "\n",
        "#Print frequency of uinque values in column  'Score'\n",
        "print (sql_df.Score.value_counts())\n",
        "print (sql_df.groupby('Score').agg('count'))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
            "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
            "      dtype='object')\n",
            "[5 1 4 2 3]\n",
            "5    363122\n",
            "4     80655\n",
            "1     52268\n",
            "3     42640\n",
            "2     29769\n",
            "Name: Score, dtype: int64\n",
            "           Id  ProductId  UserId  ...    Time  Summary    Text\n",
            "Score                             ...                         \n",
            "1       52268      52268   52268  ...   52268    52268   52268\n",
            "2       29769      29769   29769  ...   29769    29769   29769\n",
            "3       42640      42640   42640  ...   42640    42640   42640\n",
            "4       80655      80655   80655  ...   80655    80655   80655\n",
            "5      363122     363122  363122  ...  363122   363122  363122\n",
            "\n",
            "[5 rows x 9 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3q76bGxUKaS",
        "colab_type": "text"
      },
      "source": [
        "if we see the Score column, it has values [5 1 4 2 3] . Considering 1, 2 as Negative reviews and 4, 5 as Positive reviews. Consider 3 as not sure catgory and eliminate from df\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDbMYliCYzgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert the ratings into positive and negative value\n",
        "y_dict = {1:0, 2:0, 4:1, 5:1}\n",
        "sql_df= sql_df['Score'].map(y_dict) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGbJ2VQ1rabb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (sql_df.Score.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugRC75lQb1LV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#delete rows of df with 'Score' = '3' \n",
        "df = sql_df[sql_df['Score'] != 3]                       #recommended, applicable on and after pandas 0.13\n",
        "#df = sql_df.drop(sql_df['Score'] == 3 , inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW7pro7GXzUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (df.Score.value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX85MxWqsKj9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0144275c-867b-4aef-d894-2706137ff886"
      },
      "source": [
        "# removing duplicates \n",
        "df = df.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"})\n",
        "print(df.shape)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(364171, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpkGV4ejura4",
        "colab_type": "text"
      },
      "source": [
        "We have seen that HelpfulnessNumerator should always be less than or equal to HelpfulnessDenominator so checking this condition and removing those records also."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83HohwAPu36X",
        "colab_type": "text"
      },
      "source": [
        "HelfulnessNumerator says about number of people found that review usefull and HelpfulnessDenominator is about usefull review count + not so usefull count."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7bvjyuhs3s9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3bd6ab92-2583-4279-fd45-76d901978e3b"
      },
      "source": [
        "df = df[df['HelpfulnessNumerator'] <= df['HelpfulnessDenominator']]\n",
        "print(df.shape)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(364171, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gpLJu2vtf9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "user_comment = df['Text']\n",
        "user_rating = df['Score']"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}